import os
import json
import random
import time
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import google.generativeai as genai

# APIã‚­ãƒ¼è¨­å®š
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
GOOGLE_SEARCH_CX = os.environ.get("GOOGLE_SEARCH_CX")

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel("models/gemini-1.5-flash")

# HTMLæœ¬æ–‡å–å¾—
def fetch_webpage_text(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in soup(["script", "style"]):
            tag.decompose()
        return soup.get_text(separator="\\n", strip=True)
    except Exception as e:
        return f"[æœ¬æ–‡å–å¾—å¤±æ•—: {e}]"

# Googleæ¤œç´¢ + ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
def search_google_with_scrape(country):
    search_terms = ["ãƒ‘ãƒ“ãƒªã‚ªãƒ³", "é£Ÿäº‹", "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼", "ã‚¤ãƒ™ãƒ³ãƒˆ"]
    all_html = {}
    summaries = []

    for term in search_terms:
        query = f"ä¸‡åš2025 {country} {term}"
        params = {
            "key": GOOGLE_API_KEY,
            "cx": GOOGLE_SEARCH_CX,
            "q": query,
            "num": 3
        }
        response = requests.get("https://www.googleapis.com/customsearch/v1", params=params)
        print(f"[æ¤œç´¢] {query} | Status: {response.status_code}")
        if response.status_code != 200:
            continue
        results = response.json().get("items", [])
        for item in results:
            url = item.get("link")
            title = item.get("title")
            body = fetch_webpage_text(url)
            if len(body) > 100:
                all_html[title] = {
                    "url": url,
                    "text": body[:3000]
                }
                summaries.append(f"ã€{title}ã€‘\\n{body[:1000]}")

    # HTMLæœ¬æ–‡ä¿å­˜
    os.makedirs("search_logs/html_texts", exist_ok=True)
    with open(f"search_logs/html_texts/{country}.json", "w", encoding="utf-8") as f:
        json.dump(all_html, f, ensure_ascii=False, indent=2)

    return "\\n\\n".join(summaries)

# Geminiè¦ç´„
def generate_summary_from_html(text):
    try:
        prompt = f"ä»¥ä¸‹ã¯è¤‡æ•°ã®Webãƒšãƒ¼ã‚¸æœ¬æ–‡ã§ã™ã€‚ä¸‡åš2025ã«ãŠã‘ã‚‹å„å›½ãƒ‘ãƒ“ãƒªã‚ªãƒ³ã®ç‰¹å¾´ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\\n\\n{text}"
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"[Geminiè¦ç´„å¤±æ•—: {e}]"

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
def main():
    with open("all_countries.json", "r", encoding="utf-8") as f:
        countries = json.load(f)

    output_data = {}
    for country in countries:
        print(f"ğŸŸ¡ {country} ã‚’å‡¦ç†ä¸­...")
        body_text = search_google_with_scrape(country)
        summary = generate_summary_from_html(body_text)

        # Markdown å‡ºåŠ›ï¼ˆã“ã“ãŒæ­£ã—ã„ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆï¼‰
        md_dir = "_posts"
        os.makedirs(md_dir, exist_ok=True)
        md_filename = os.path.join(md_dir, f"{datetime.now().strftime('%Y-%m-%d')}-{country}.md")
        with open(md_filename, "w", encoding="utf-8") as f:
            f.write(f"# {country}ã®ãƒ‘ãƒ“ãƒªã‚ªãƒ³è¦ç´„\\n\\n")
            f.write(f"**ç”Ÿæˆæ—¥æ™‚ï¼š** {datetime.now().isoformat()}\\n\\n")
            f.write(summary)

        output_data[country] = {
            "summary": summary,
            "timestamp": datetime.now().isoformat()
        }
        time.sleep(2)

    with open("used_pavilion_gemini.json", "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()
